{"paragraphs":[{"text":"%md\n\n# Prise en main de PySpark\n\nCeci est un support de cours ;-). \n\n\n## Les Resilients distributed datasets (RDD). Kesako?\n\nUn RDD est une collection partitionnée d’enregistrements en lecture seule qui ne peut être créée que par des opérations déterministes. En gros, analogiquement, c'est une liste partitionnée dont les morceaux vivent dans la RAM des différents **workers** spark d'un cluster. ","user":"anonymous","dateUpdated":"2019-08-20T13:16:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Prise en main de PySpark</h1>\n<p>Ceci est un support de cours ;-).</p>\n<h2>Les Resilients distributed datasets (RDD). Kesako?</h2>\n<p>Un RDD est une collection partitionnée d’enregistrements en lecture seule qui ne peut être créée que par des opérations déterministes. En gros, analogiquement, c'est une liste partitionnée dont les morceaux vivent dans la RAM des différents <strong>workers</strong> spark d'un cluster.</p>\n"}]},"apps":[],"jobName":"paragraph_1566306513076_192871689","id":"20190820-130833_2024879513","dateCreated":"2019-08-20T13:08:33+0000","dateStarted":"2019-08-20T13:16:15+0000","dateFinished":"2019-08-20T13:16:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5478"},{"text":"%spark2.pyspark\n\n# la methode parallelize converti la liste en un RDD\nwords = sc.parallelize (\n   [\"scala\", \n   \"java\", \n   \"hadoop\", \n   \"spark\", \n   \"akka\",\n   \"spark vs hadoop\", \n   \"pyspark\",\n   \"pyspark and spark\"]\n)\n","user":"anonymous","dateUpdated":"2019-08-20T14:07:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1566305129381_-328381864","id":"20190820-124529_740218968","dateCreated":"2019-08-20T12:45:29+0000","dateStarted":"2019-08-20T14:07:47+0000","dateFinished":"2019-08-20T14:07:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5479"},{"text":"%spark2.pyspark\n\n# notez que l'execution de la methode count initie un spark job (normal le RDD est un objet distribué)\ncounts = words.count()\nprint \"Number of elements in RDD -> %i\" % (counts)","user":"anonymous","dateUpdated":"2019-08-20T14:07:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Number of elements in RDD -> 8\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=7"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1566305607813_-1962163020","id":"20190820-125327_735803878","dateCreated":"2019-08-20T12:53:27+0000","dateStarted":"2019-08-20T14:07:51+0000","dateFinished":"2019-08-20T14:07:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5480"},{"text":"%spark2.pyspark\n\n# notez aussi qu'un RDD N'EST PAS UNE LISTE au sens de python. La preuve\nprint words[2]","user":"anonymous","dateUpdated":"2019-08-20T13:13:39+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 1: print words[2]\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-6097419831649483967.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'RDD' object does not support indexing\n"}]},"apps":[],"jobName":"paragraph_1566305797708_1942921093","id":"20190820-125637_963801887","dateCreated":"2019-08-20T12:56:37+0000","dateStarted":"2019-08-20T13:01:43+0000","dateFinished":"2019-08-20T13:01:43+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5481"},{"text":"%spark2.pyspark \n\n# Pour récupérer le contenu d'un RDD (i.e. le désérialiser) on se sert de la methode collect (qui initie également un spark job)\nprint \"Elements in RDD -> {}\".format(words.collect())\n","user":"anonymous","dateUpdated":"2019-08-20T13:07:30+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Elements in RDD -> ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=2"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1566306060678_-77642757","id":"20190820-130100_379142720","dateCreated":"2019-08-20T13:01:00+0000","dateStarted":"2019-08-20T13:06:57+0000","dateFinished":"2019-08-20T13:06:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5482"},{"text":"%md\n\n## Effectuer des traitements parallèles à l'aide de Map / Reduce / Filter\n\nCe que nous allons montrer ici n'est pas très différent de la **programmation fonctionnelle** locale telle qu'on peut la faire en Python ou R. La seule différence tient ici à la parallélisation\n\n[Un bon article sur la PF en python](http://sametmax.com/map-filter-et-reduce/)\n\n","user":"anonymous","dateUpdated":"2019-08-20T13:24:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Effectuer des traitements parallèles à l'aide de Map / Reduce / Filter</h2>\n<p>Ce que nous allons montrer ici n'est pas très différent de la <strong>programmation fonctionnelle</strong> locale telle qu'on peut la faire en Python ou R. La seule différence tient ici à la parallélisation</p>\n<p><a href=\"http://sametmax.com/map-filter-et-reduce/\">Un bon article sur la PF en python</a></p>\n"}]},"apps":[],"jobName":"paragraph_1566306417459_-1225828978","id":"20190820-130657_490899860","dateCreated":"2019-08-20T13:06:57+0000","dateStarted":"2019-08-20T13:24:32+0000","dateFinished":"2019-08-20T13:24:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5483"},{"text":"%spark2.pyspark \n\n# Regardons déjà comment cela se présente en python local. J'ai une liste de nom dont je veux récupérer tous les noms de plus de 5 lettres\nnoms = [\"Marie\", \"Jean\", \"Pierre\" , \"Bethsabee\"]\n\n# On peut adopter une approche itérative (i.e. boucler sur la structure)\nnoms_longs = []\nfor n in noms: \n    if len(n)>5:\n        noms_longs.append(n)\nprint noms_longs\n","user":"anonymous","dateUpdated":"2019-08-20T13:23:43+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"['Pierre', 'Bethsabee']\n"}]},"apps":[],"jobName":"paragraph_1566306502543_1571534382","id":"20190820-130822_636896854","dateCreated":"2019-08-20T13:08:22+0000","dateStarted":"2019-08-20T13:23:43+0000","dateFinished":"2019-08-20T13:23:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5484"},{"text":"%spark2.pyspark\n\n# ou alors filter en utilisant une fonction\nnoms_longs = list(filter(lambda x: len(x)>5, noms))\nprint noms_longs","user":"anonymous","dateUpdated":"2019-08-20T13:26:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"['Pierre', 'Bethsabee']\n"}]},"apps":[],"jobName":"paragraph_1566307412110_1064045811","id":"20190820-132332_563912672","dateCreated":"2019-08-20T13:23:32+0000","dateStarted":"2019-08-20T13:26:24+0000","dateFinished":"2019-08-20T13:26:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5485"},{"text":"%spark2.pyspark\n# revenons maintenant à notre RDD et récupérons les noms de plus de 5 caractères à l'aide de FILTER\n\nlong_words = words.filter(lambda x: len(x)>5)\nprint \"les mots de plus de 5 caractères: {}\".format(long_words.collect())","user":"anonymous","dateUpdated":"2019-08-20T13:44:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"les mots de plus de 5 caractères: ['hadoop', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=3"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1566307584207_-672540370","id":"20190820-132624_383659053","dateCreated":"2019-08-20T13:26:24+0000","dateStarted":"2019-08-20T13:29:56+0000","dateFinished":"2019-08-20T13:29:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5486"},{"text":"%spark2.pyspark\n\n# remplaçons tous les espaces par un _ à l'aide de MAP\n\nwords_without_spaces = words.map(lambda x: x.replace(\" \", \"_\"))\nprint \"les mots sans espaces: {}\".format(words_without_spaces.collect())","user":"anonymous","dateUpdated":"2019-08-20T13:43:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"les mots sans espaces: ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark_vs_hadoop', 'pyspark', 'pyspark_and_spark']\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=4"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1566307796143_1057614581","id":"20190820-132956_1752880610","dateCreated":"2019-08-20T13:29:56+0000","dateStarted":"2019-08-20T13:43:25+0000","dateFinished":"2019-08-20T13:43:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5487"},{"text":"%spark2.pyspark\nfrom operator import add\n\n# enfin concatenons l'ensemble des mots en une seule str à l'aide de REDUCE\n\nconcatenated = words.reduce(add)\nprint \"les mots concatene: {}\".format(concatenated)","user":"anonymous","dateUpdated":"2019-08-20T14:25:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"les mots concatene: scalajavahadoopsparkakkaspark vs hadooppysparkpyspark and spark\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=16"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1566308605218_655718019","id":"20190820-134325_1737989309","dateCreated":"2019-08-20T13:43:25+0000","dateStarted":"2019-08-20T14:25:38+0000","dateFinished":"2019-08-20T14:25:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5488"},{"text":"%spark2.pyspark\n\n#TODO: à la suite de la fusion des boites x et y, constituez une équipe composée: de tous les Products Owner (po) et des devs de plus de 30/ \n\n# NB 1) on peut définir un RDD comme la somme de deux RDD ou plus à l'aide de \"+\"\"\n#    2) rien n'empèche d'appliquer plusieurs map / filter successifs\n\nboite_x = sc.parallelize([(\"Claire\", 37, \"dev\"),(\"Pierre\", 24, \"po\"),(\"Marie\",22, \"dev\")])\nboite_y = sc.parallelize([(\"Charles\", 33, \"po\"),(\"Ali\", 23, \"dev\"),(\"Karl\", 40, \"dev\")])\n\n# ICI\nequipe = (boite_x + boite_y).filter(lambda x: x[2]==\"po\") + (boite_x + boite_y).filter(lambda x: x[2]==\"dev\").filter(lambda x: x[1]>30)\nprint equipe.collect()\n","user":"anonymous","dateUpdated":"2019-08-20T14:35:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[('Pierre', 24, 'po'), ('Charles', 33, 'po'), ('Claire', 37, 'dev'), ('Karl', 40, 'dev')]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=19"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1566308803000_351553321","id":"20190820-134643_787654226","dateCreated":"2019-08-20T13:46:43+0000","dateStarted":"2019-08-20T14:33:20+0000","dateFinished":"2019-08-20T14:33:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5489"},{"text":"%spark2.pyspark\n","user":"anonymous","dateUpdated":"2019-08-20T14:15:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566310523951_-447659393","id":"20190820-141523_1592752361","dateCreated":"2019-08-20T14:15:23+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:5490"}],"name":"first_pyspark","id":"2EMDKC514","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}